{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f01994",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c49af9",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique used to scale the values of a feature to a specific range, typically between 0 and 1. This is done by subtracting the minimum value of the feature from each data point, and then dividing by the range of the feature. This can help improve the performance of machine learning models by ensuring that all features have the same scale.\n",
    "\n",
    "Here's an example of how Min-Max scaling can be applied to a dataset using Python's scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9052056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and Transform the data using the scaler\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Print the scaled data\n",
    "print(scaled_data)\n",
    "\n",
    "\n",
    "#This code creates a sample dataset with two features and \n",
    "#applies Min-Max scaling to scale the values of each feature to the range [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de966a4",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25111b5",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Normalization or L2 normalization, is a feature scaling method that scales the values of a feature by dividing each value by the magnitude of the feature vector. This results in a new feature vector with a magnitude of 1, where each value represents the cosine of the angle between the original feature vector and the unit vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa56c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4472136  0.89442719]\n",
      " [0.6        0.8       ]\n",
      " [0.6401844  0.76822128]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Create a Normalizer object\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Fit and Transform the data using the normalizer\n",
    "normalized_data = normalizer.fit_transform(data)\n",
    "\n",
    "# Print the normalized data\n",
    "print(normalized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0a0b9",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f25d30",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique that is often used to reduce the dimensionality of large datasets by transforming a large set of variables into a smaller one that still contains most of the information in the large set. This is done by finding the principal components of the data, which are new variables that are linear combinations of the original variables and capture most of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf59ed2",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f253a6e",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a technique that can be used for feature extraction, which is the process of transforming the input data into a set of new, more informative features. PCA works by finding the principal components of the data, which are new variables that are linear combinations of the original variables and capture most of the variance in the data. These principal components can be used as new features in a machine learning model, as they contain most of the information in t Is there anything else you would like to know?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7bfa53",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e542b6",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique that can be used to scale the values of the features in the dataset to a specific range, typically between 0 and 1. This can help improve the performance of the recommendation system by ensuring that all features have the same scale and that no single feature dominates the others.\n",
    "\n",
    "To apply Min-Max scaling to the food delivery dataset, you would first need to identify the minimum and maximum values for each feature, such as price, rating, and delivery time. Then, for each data point, you would subtract the minimum value of the feature from the data pointâ€™s value for that feature, and then divide by the range of the feature (i.e., the difference between the maximum and minimum values). This would scale the values of each feature to the range [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3228089f",
   "metadata": {},
   "source": [
    "## Q6. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00a1c05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    v\n",
       "0   1\n",
       "1   5\n",
       "2  10\n",
       "3  15\n",
       "4  20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "value = [1, 5, 10, 15, 20]\n",
    "value = pd.DataFrame(value , columns=['v'])\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dac1d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit_transform(value[['v']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf92ee4c",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658456bf",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on a dataset containing the following features: [height, weight, age, gender, blood pressure], we need to follow these steps:\n",
    "\n",
    "- Standardize the data: Since PCA is sensitive to the scale of the features, we need to normalize them to have zero mean and unit variance. This can be done using the StandardScaler class from sklearn.preprocessing module.\n",
    "<br>\n",
    "- Fit and transform the data using PCA: We can use the PCA class from sklearn.decomposition module to perform PCA on the standardized data. We can specify the number of components we want to retain, or let the PCA class choose the optimal number based on a given variance threshold. The PCA class will return a new array with the principal components as the new features.\n",
    "<br>\n",
    "- Interpret the results: The PCA class will also provide some attributes that can help us understand the results of PCA, such as explained_variance_ratio_, which shows the percentage of variance explained by each principal component, and components_, which shows the linear combination of the original features that form each principal component.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
