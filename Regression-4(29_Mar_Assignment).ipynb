{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e9db14",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211fada",
   "metadata": {},
   "source": [
    "- Lasso Regression is a type of linear regression that uses L1 regularization to shrink the coefficients of the input features towards zero, resulting in a sparse model that has fewer non-zero coefficients.\n",
    "- It differs from other regression techniques such as ordinary least squares (OLS), which does not use regularization, or Ridge Regression, which uses L2 regularization to shrink the coefficients towards zero, but does not produce a sparse model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883a682c",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038a7ad",
   "metadata": {},
   "source": [
    "\n",
    "- The main advantage of using Lasso Regression in feature selection is that it can **automatically select the most relevant features** for the prediction task by setting the coefficients of the less important features to zero .\n",
    "- This can help to **reduce the dimensionality** of the input data, **improve the interpretability** of the model, and **avoid overfitting** .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcbcd41",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b19f36",
   "metadata": {},
   "source": [
    "- The coefficients of a Lasso Regression model represent the **effect of each input feature on the output variable**, holding all other features constant.\n",
    " - A positive coefficient means that the output variable increases as the input feature increases, while a negative coefficient means that the output variable decreases as the input feature decreases.\n",
    " - A zero coefficient means that the input feature has **no effect** on the output variable, and it is **excluded** from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63813dc8",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa72c1",
   "metadata": {},
   "source": [
    "- The main tuning parameter that can be adjusted in Lasso Regression is the **regularization parameter (lambda)**, which controls the amount of shrinkage applied to the coefficients.\n",
    " - A larger lambda means more shrinkage, resulting in a **simpler model** with fewer non-zero coefficients, but also a higher **bias** and a lower **variance** .\n",
    " - A smaller lambda means less shrinkage, resulting in a **more complex model** with more non-zero coefficients, but also a lower **bias** and a higher **variance** .\n",
    "- The optimal value of lambda depends on the **trade-off** between bias and variance, and it can be determined by using **cross-validation** or other methods ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e7c99",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1802da4",
   "metadata": {},
   "source": [
    "- Yes, Lasso Regression can be used for non-linear regression problems by using **polynomial features** or other **non-linear transformations** of the input data .\n",
    " - Polynomial features are created by raising the input features to different powers or by multiplying them together .\n",
    " - Non-linear transformations are functions that map the input features to a different space, such as **logarithmic**, **exponential**, **sine**, or **cosine** functions .\n",
    " - By using these methods, Lasso Regression can capture the **non-linear relationships** between the input and output variables, while still applying the **L1 regularization** to select the most relevant features ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c1da6",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09159a43",
   "metadata": {},
   "source": [
    "- Ridge Regression and Lasso Regression are both types of linear regression that use **regularization** to shrink the coefficients of the input features towards zero, but they use different types of regularization .\n",
    " - Ridge Regression uses **L2 regularization**, which adds the **squared** value of the coefficients to the loss function, resulting in a **penalty** that is proportional to the **magnitude** of the coefficients .\n",
    " - Lasso Regression uses **L1 regularization**, which adds the **absolute** value of the coefficients to the loss function, resulting in a **penalty** that is proportional to the **sign** of the coefficients .\n",
    " - The main difference between Ridge Regression and Lasso Regression is that Ridge Regression **does not produce a sparse model**, while Lasso Regression **does** .\n",
    "- This means that Ridge Regression **keeps all the input features** in the model, but with **smaller coefficients**, while Lasso Regression **discards some of the input features** by setting their coefficients to **zero** ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f13c7a",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3f68dc",
   "metadata": {},
   "source": [
    "- Yes, Lasso Regression can handle multicollinearity in the input features by using the **L1 regularization** to select the most relevant features and exclude the redundant ones  .\n",
    "- Multicollinearity is a situation where some of the input features are **highly correlated** with each other, which can cause **instability** and **inaccuracy** in the regression coefficients  .\n",
    "- Lasso Regression can handle multicollinearity by applying a **penalty** to the coefficients that is proportional to their **sign**, which forces some of the coefficients to become **zero**  .\n",
    "- This way, Lasso Regression can **eliminate** the multicollinear features from the model, and **retain** only the features that have a **strong and unique** effect on the output variable  ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff682b0a",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53754fd",
   "metadata": {},
   "source": [
    " - The optimal value of the regularization parameter (lambda) in Lasso Regression is the one that **minimizes** the **prediction error** on the **validation data**  .\n",
    "- The prediction error is the difference between the **actual** and the **predicted** values of the output variable, measured by a **loss function** such as **mean squared error (MSE)** or **mean absolute error (MAE)**  .\n",
    "- The validation data is a subset of the **available data** that is **not used** for training the model, but for **evaluating** its performance  .\n",
    "- The optimal value of lambda can be found by using **cross-validation** or other methods, such as **grid search** or **coordinate descent**  .\n",
    "- Cross-validation is a technique that **splits** the available data into **k** folds, and uses **k-1** folds for training and **1** fold for validation, and repeats this process for **k** times, each time using a different fold for validation  .\n",
    " - Grid search is a technique that **tries** different values of lambda in a **predefined range**, and selects the one that gives the **lowest** prediction error on the validation data  .\n",
    " - Coordinate descent is a technique that **updates** the value of lambda by **moving** along the **direction** that **reduces** the prediction error on the validation data, until it reaches a **local minimum**  .\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
